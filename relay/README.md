# Relay Package

MoQT (Media over QUIC Transport) relay implementation with optimized broadcast patterns.

## Architecture

### Core Components

- **handler.go** - Main relay handler with trackDistributor (Broadcast Channel pattern)
- **group_cache.go** - Ring buffer for group caching with atomic operations
- **frame_pool.go** - sync.Pool-based frame allocation for memory efficiency
- **config.go** - Configuration structures

### Broadcast Pattern

Implements **Broadcast Channel** pattern for optimal performance with 10-300 concurrent subscribers:
- Each subscriber gets a dedicated notification channel
- 1ms timeout for optimal CPU/latency tradeoff
- Zero allocations during broadcast
- Thread-safe subscribe/unsubscribe operations

**Performance Characteristics** (100 subscribers):
- Latency: ~800ns per broadcast
- CPU Usage: ~1% (vs 200% for atomic polling)
- Memory: 3.5MB/s idle (with 1ms timeout)
- Zero allocations during steady state

## Test Structure

### Test Files

#### `distributor_test.go` (åŸºæœ¬ãƒ†ã‚¹ãƒˆ)
Tests for trackDistributor and broadcast mechanism:
- âœ… Broadcast to all subscribers (critical bug fix verification)
- âœ… Subscribe/unsubscribe lifecycle
- âœ… Concurrent access safety
- âœ… Channel buffering behavior
- âœ… Non-blocking broadcasts
- âœ… Memory leak prevention
- ğŸ”¬ Benchmarks: Broadcast (10/100/500 subs), Subscribe ops

#### `distributor_advanced_test.go` (é«˜åº¦ãªãƒ†ã‚¹ãƒˆ - NEW)
Advanced distributor tests:
- âœ… Edge cases (empty distributor, nonexistent channels, rapid churn)
- âœ… Stress testing (high frequency broadcasts, subscriber churn)
- âœ… Scalability testing (1-1000 subscribers)
- âœ… Memory behavior verification
- âœ… Race condition detection
- âœ… Notification delivery guarantees
- ğŸ”¬ Benchmark: Variable load patterns

#### `group_cache_test.go` (åŸºæœ¬ãƒ†ã‚¹ãƒˆ)
Tests for ring buffer and group caching:
- âœ… Frame appending and cloning
- âœ… Frame retrieval by index
- âœ… Ring buffer head/tail tracking
- âœ… Earliest available group calculation
- âœ… Wrap-around behavior
- âœ… Concurrent read/write access
- ğŸ”¬ Benchmarks: Cache append, ring get

#### `group_cache_advanced_test.go` (é«˜åº¦ãªãƒ†ã‚¹ãƒˆ - NEW)
Advanced group cache tests:
- âœ… Edge cases (empty cache, overflow, boundary conditions)
- âœ… Capacity handling (default/custom)
- âœ… Cache behavior (empty/large/many frames, independence)
- âœ… Stress testing (rapid updates, concurrent operations)
- âœ… Memory behavior
- âœ… Sequence number handling (sequential/non-sequential)
- ğŸ”¬ Benchmarks: Sequential append, random access, concurrent operations

#### `frame_pool_test.go` (åŸºæœ¬ãƒ†ã‚¹ãƒˆ)
Tests for frame pooling:
- âœ… Pool get/put operations
- âœ… Frame reset on return
- âœ… Concurrent access
- âœ… Multiple pool instances
- âœ… Frame reuse verification
- ğŸ”¬ Benchmarks: Pool operations, reuse vs no-reuse

#### `frame_pool_advanced_test.go` (é«˜åº¦ãªãƒ†ã‚¹ãƒˆ - NEW)
Advanced frame pool tests:
- âœ… Edge cases (empty pool, nil frames, multiple puts)
- âœ… Stress testing (high frequency, imbalanced get/put)
- âœ… Memory efficiency verification
- âœ… Capacity variations (100-10000 bytes)
- âœ… Reset behavior details
- âœ… Concurrent patterns (producer-consumer, burst load)
- âœ… Pool isolation verification
- âœ… Reuse rate statistics
- ğŸ”¬ Benchmarks: Realistic patterns, pool vs naive comparison

## Running Tests

### All Tests
```bash
# Standard run
go test -v

# With coverage
go test -cover

# Skip stress tests (faster)
go test -short
```

### Benchmarks
```bash
# All benchmarks
go test -bench=. -benchmem -run=^$

# Quick benchmarks (100ms each)
go test -bench=. -benchmem -benchtime=100ms -run=^$

# Specific categories
go test -bench=BenchmarkBroadcast -benchmem -run=^$
go test -bench=BenchmarkFramePool -benchmem -run=^$
go test -bench=BenchmarkGroupCache -benchmem -run=^$
```

### Specific Test Categories
```bash
# Distributor tests
go test -run TestDistributor -v

# Advanced/edge case tests
go test -run "Advanced|Edge|Stress" -v

# Frame pool tests
go test -run TestFramePool -v

# Group cache tests
go test -run TestGroupCache -v
go test -run TestGroupRing -v
```

### Performance Testing
```bash
# Run with race detector
go test -race

# CPU profiling
go test -cpuprofile=cpu.prof -bench=.

# Memory profiling
go test -memprofile=mem.prof -bench=.
```

## Test Results

**Current Status**: âœ… All tests passing

**Test Coverage**:
- **Total Tests**: 42 unit tests + 1 skipped (race condition documentation)
- **Benchmarks**: 23 performance benchmarks
- **Coverage**: 25.9% of statements (expected - many paths require integration)

**Test Categories**:
- Basic functionality: 24 tests
- Advanced/edge cases: 18 tests
- Stress tests: 3 tests (run with `-short` to skip)
- Skipped tests: 1 (documents non-thread-safe behavior)

**Benchmark Summary** (100ms runtime):
```
Distributor:
  BroadcastVariableLoad          8,929 ns/op    (50% active subscribers)
  Broadcast_10                     180 ns/op    0 allocs
  Broadcast_100                  1,290 ns/op    0 allocs
  Broadcast_500                  5,939 ns/op    0 allocs
  Subscribe                        176 ns/op    1 allocs

Frame Pool:
  FramePoolGet                      13 ns/op    0 allocs
  FramePoolWithReuse                16 ns/op    0 allocs
  FramePoolNoReuse                 855 ns/op    1 allocs  (54x slower!)
  FramePoolConcurrent                7 ns/op    0 allocs
  Pool vs Naive (write)             19 ns vs 553 ns  (29x faster)

Group Cache:
  GroupCacheAppend               1,359 ns/op    2 allocs
  GroupRingGet                       9 ns/op    0 allocs
  SequentialAppend               3,241 ns/op
  RandomAccess                     0.3 ns/op    (CPU cache hit)
  Head/EarliestRead              0.4-0.6 ns/op  (atomic read)
  ConcurrentGetHead                  3 ns/op
```

## Design Decisions

### Why Broadcast Channel over Atomic Epoch?

Comprehensive benchmarking (see `../benchmarks/fanout/`) showed:

**Broadcast Channel** (Winner for MoQT):
- âœ… Sufficient latency: 1.5Âµs << 10ms target
- âœ… Low CPU: 1% at 100 subscribers
- âœ… Acceptable memory: 3.5MB/s with 1ms timeout
- âœ… Handles QUIC blocking gracefully

**Atomic Epoch** (Rejected):
- âœ… Excellent latency: 50ns (constant)
- âŒ Extreme CPU: 200% at 100 subscribers (200x worse)
- âœ… Minimal memory: 10KB/s
- âŒ Requires continuous polling

**Crossover Point**: ~30 subscribers (where costs become equal)

**MoQT Reality**: 10-300 subscribers â†’ Broadcast Channel optimal

### Why Not Pion's Synchronous Loop?

Pion WebRTC uses RWMutex-protected synchronous fanout:
- Works for WebRTC: Non-blocking UDP writes
- **Fails for MoQT**: QUIC streams can block on flow control
- Benchmark showed 186x slowdown with 10% slow subscribers
- Broadcast allows independent subscriber goroutines

## Integration Notes

When using this package:

1. **Timeout Configuration**: `NotifyTimeout = 1ms` (optimized via benchmarks)
2. **Ring Size**: `GroupCacheCount = 8` (configurable)
3. **Frame Capacity**: `DefaultFrameCapacity = 1500` (MTU-sized)
4. **Thread Safety**: All operations are thread-safe
5. **Resource Management**: Always defer `unsubscribe()` after `subscribe()`

## Future Improvements

- [ ] Integration tests with real moqt.TrackReader/Writer
- [ ] Adaptive timeout based on subscriber count
- [ ] Metrics/observability hooks
- [ ] Configurable broadcast channel buffer size
- [ ] Dead subscriber detection and cleanup
